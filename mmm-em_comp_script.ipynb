{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540228c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "## 2021-10-25  Expectation Maximization for Multinomial mixture model estimation using Python\n",
    "## Reference:\n",
    "## Shenhav, L., Thompson, M., Joseph, T.A. et al. FEAST: fast expectation-maximization for microbial source tracking. Nat Methods 16, 627–632 (2019). \n",
    "##https://doi.org/10.1038/s41592-019-0431-x\n",
    "\n",
    "\n",
    "def run_EM(countfile,metadata,iteration):\n",
    "## preparation for data: source count matrix(with unknown source initiation,for population simulation),sink count matrix(vector),\n",
    "## observed source count matrix (fill unknown source count matrix with 0) , initial proportion of sources generated by Uniform Distribution\n",
    "    filename = metadata.split(\"\\\\\")[-1].replace(\"metadata.txt\",\"\")\n",
    "    print(f'{filename} estimation started\\n')\n",
    "    wb = pd.read_csv(countfile,sep=\"\\t\",header=0,index_col = 0)\n",
    "    df = pd.DataFrame(wb)\n",
    "    mwb = pd.read_csv(metadata,sep=\"\\t\",header = 0,index_col = 0)\n",
    "    mdf = pd.DataFrame(mwb)\n",
    "    ## labeling all sink samples and source samples\n",
    "    source_idx = list(mdf[(mdf['SourceSink']=='Source')].index)\n",
    "    sink_idx = list(mdf[(mdf['SourceSink']=='Sink')].index)\n",
    "    taxa_idx = list(df.index)\n",
    "    sink_name = sink_idx[0]\n",
    "    source_matrix = np.array(df[source_idx])\n",
    "    sink_matrix = np.array(df[sink_idx])\n",
    "    ob_unknown = [0 for x in range(len(sink_matrix))]\n",
    "    ob_source_matrix = np.c_[source_matrix,np.array(ob_unknown)]\n",
    "    source_matrix = unknown_sources_init(sink_matrix,source_matrix)\n",
    "    print(\"unknown source initiation completed\")\n",
    "    alpha = np.random.uniform(low=0,high=1,size=(source_matrix.shape[1],1))\n",
    "    alpha = np.array([x/np.sum(alpha) for x in alpha])\n",
    "    alpha1 = alpha\n",
    "    ## estimate latent variable alpha & dist params by iteration\n",
    "    param_est = [alpha1[0]]\n",
    "    for i in range(1,iteration+1):\n",
    "        alpha2 = E_step_refined(source_matrix,alpha1)\n",
    "        new_res = M_step_refined(source_matrix,sink_matrix,alpha2,ob_source_matrix)\n",
    "        alpha1 = new_res['alpha']\n",
    "        source_matrix = new_res['params']\n",
    "        param_est.append(alpha1[0])\n",
    "        print(f'round {i} completed\\n')\n",
    "        print(abs(param_est[i]-param_est[i-1]))\n",
    "        if abs(param_est[i]-param_est[i-1])< 1e-6:\n",
    "            print(\"iteration convergence has come\\n\")\n",
    "            break\n",
    "\n",
    "    sink_type = mdf[(mdf['SourceSink']=='Sink')]['Type'].tolist()\n",
    "    new_res['sink_type'] = sink_type\n",
    "    new_res['sink_name'] = sink_name\n",
    "    new_res['sample'] = source_idx\n",
    "    new_res['taxa'] = taxa_idx\n",
    "    return(new_res)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def unknown_sources_init(sink_cm,source_cm):\n",
    "    ## first asign general taxa abundance to unknown source by:\n",
    "    ## unknown_taxa j = (sink[j]-sum(all_sources[j]))(taxa j >= 0)\n",
    "    ## second , find all core taxa(present in over 50% sources)，fill the core taxa in unknown sources by:\n",
    "    ## unknown_taxa[which is core taxa] = min(all_sources[core_taxa])/2\n",
    "    ## finally , adjust the value of taxas which are absent in all known sources by:\n",
    "    ## unknown_taxa[no presence taxa] = sink[np_taxa]-poison(1,0.5)(np_taxa >= 0)\n",
    "    unknown_taxa = []\n",
    "    basestone_taxa_idx = []\n",
    "    absent_taxa_idx = []\n",
    "    for j in range(sink_cm.shape[0]):\n",
    "        unknown_taxa.append(max(float(sink_cm.squeeze()[j]-np.sum(source_cm[j])),0))\n",
    "        if len(source_cm[j][source_cm[j]>0]) > (len(source_cm[j])/2):\n",
    "            basestone_taxa_idx.append(j)\n",
    "        if len(source_cm[j][source_cm[j]==0]) == len(source_cm[j]):\n",
    "            absent_taxa_idx.append(j)\n",
    "    for taxa in basestone_taxa_idx:\n",
    "        unknown_taxa[taxa] = min(source_cm[taxa])/2\n",
    "    for taxa in absent_taxa_idx:\n",
    "        unknown_taxa[taxa] = max(sink_cm.squeeze()[taxa]-np.random.poisson(lam=0.5),0)\n",
    "    print(len(unknown_taxa))\n",
    "    print(np.array(unknown_taxa).shape)\n",
    "    new_source_cm = np.c_[source_cm,np.array(unknown_taxa)]\n",
    "    print(new_source_cm.shape)\n",
    "    return(new_source_cm)\n",
    "\n",
    "\n",
    "def E_step_refined(source_sample,alpha):\n",
    "    ## expect conditional probability of (P(i|j)) by:\n",
    "    ## JΣP(i|j) = JΣαi*γij / KΣJΣαi*γij\n",
    "    numerator_list = []\n",
    "    new_alpha = []\n",
    "    for i in range(source_sample.shape[1]):\n",
    "        #numerator = [source_sample[i,j]*alpha[i] for j in range(len(source_sample[i]))]\n",
    "        numerator = (np.array(source_sample[:,i])*np.array(alpha[i])).tolist()\n",
    "        numerator_list.append(numerator)\n",
    "    numerator_array = np.transpose(np.array(numerator_list))\n",
    "    denominator = np.sum(numerator_array)\n",
    "    for numerator in numerator_list:\n",
    "        new_proport = sum(numerator)/denominator\n",
    "        new_alpha.append(new_proport)\n",
    "    new_alpha = np.array(new_alpha)\n",
    "    new_alpha = new_alpha[:,np.newaxis]\n",
    "    print(np.sum(new_alpha))\n",
    "    print(new_alpha.shape)\n",
    "    return(new_alpha)\n",
    "def M_step_refined(source_sample,sink_sample,alpha,ob_source_sample):\n",
    "    ## calculate new alpha by:\n",
    "    ## αi =  JΣxj*P(i|j)\n",
    "    new_res = dict()\n",
    "    sink_reab = np.array([np.float64(x)/np.sum(sink_sample) for x in sink_sample])\n",
    "    sink_reab = np.nan_to_num(sink_reab)\n",
    "    multi_alpha = np.array([alpha.squeeze() for x in range(source_sample.shape[0])])\n",
    "    total_denomin = source_sample*multi_alpha\n",
    "    new_alpha = []\n",
    "    new_source_sample = []\n",
    "    for i in range(source_sample.shape[1]):\n",
    "        cp1 = sink_reab.squeeze()*np.array([np.float64(source_sample[j,i])/np.sum(total_denomin[j]) for j in range(source_sample.shape[0])])\n",
    "        cp1 = np.nan_to_num(cp1)\n",
    "        cp2 = np.array([float(alpha[i]) for x in range(source_sample.shape[0])])\n",
    "        sub_numerat = cp1*cp2\n",
    "        sub_alpha = sum(sub_numerat) \n",
    "        new_alpha.append(sub_alpha)\n",
    "    new_alpha = np.array(new_alpha)\n",
    "    new_alpha = new_alpha[:,np.newaxis]\n",
    "    ## calculate new source parameter by:\n",
    "    ## γij =  xj*P(i|j)+yij/JΣ(xj*P(i|j)+yij)\n",
    "    for i in range(source_sample.shape[1]):\n",
    "        cp3 = sink_sample.squeeze()*np.array([np.float64(source_sample[j,i])/np.sum(total_denomin[j]) for j in range(source_sample.shape[0])])\n",
    "        cp3 = np.nan_to_num(cp3)\n",
    "        cp4 = np.array([float(alpha[i]) for x in range(source_sample.shape[0])])\n",
    "        sub_sub_num = cp3*cp4\n",
    "        sub_num = sub_sub_num + ob_source_sample[:,i]\n",
    "        total_sub = (sub_num/sum(sub_num))\n",
    "        total_sub = np.nan_to_num(total_sub)\n",
    "        total_sub = total_sub.tolist()\n",
    "        new_source_sample.append(total_sub)\n",
    "    new_source_sample = np.transpose(np.array(new_source_sample))\n",
    "    new_res['alpha'] = new_alpha\n",
    "    new_res['params'] = new_source_sample\n",
    "    return(new_res)\n",
    "def merge_count_matrix(filepath):\n",
    "    file_list = list(glob.glob(os.path.join(filepath,\"*.txt\")))\n",
    "    union_taxa_list = []\n",
    "    merge_table = []\n",
    "    sample_list = ['taxa']\n",
    "    \n",
    "    print(file_list)\n",
    "    ## find union of all samples' taxas\n",
    "    for file in file_list:\n",
    "        wb = pd.read_csv(file,sep=\"\\t\",header=None)\n",
    "        df = pd.DataFrame(wb)\n",
    "        switch_dict = {True:len(df.columns)-1,False:0}\n",
    "        col_name = []\n",
    "        for j in range(len(df.columns)):\n",
    "            col_name.append(df[0:1][j][0])\n",
    "        if '#ID' in col_name:\n",
    "            otu_flag = True\n",
    "        else:\n",
    "            otu_flag = False\n",
    "\n",
    "        for i in range(1,len(df)):\n",
    "            drow = df[i:i+1]\n",
    "            taxa = str(drow[switch_dict[otu_flag]][i])\n",
    "            if taxa not in union_taxa_list:\n",
    "                union_taxa_list.append(taxa)\n",
    "    merge_table.append(union_taxa_list)\n",
    "    \n",
    "    ## merge all samples into one count matrix\n",
    "    for file in file_list:\n",
    "        wb = pd.read_csv(file,sep=\"\\t\",header=None)\n",
    "        df = pd.DataFrame(wb)\n",
    "        switch_dict = {True:len(df.columns)-1,False:0}\n",
    "        col_name = []\n",
    "        for j in range(len(df.columns)):\n",
    "            col_name.append(df[0:1][j][0])\n",
    "        if '#ID' in col_name:\n",
    "            otu_flag = True\n",
    "        else:\n",
    "            otu_flag = False\n",
    "        site_taxa = set(list(df[df.columns[switch_dict[otu_flag]]]))\n",
    "        df.set_index(switch_dict[otu_flag],inplace=True)            \n",
    "        #print(site_taxa)\n",
    "\n",
    "        for j in range(1,len(df.columns)):\n",
    "            sample_list.append(df[0:1][j][0])\n",
    "            sample_table = []\n",
    "            for u_taxa in union_taxa_list:\n",
    "                if u_taxa in site_taxa:\n",
    "                    if isinstance(df.loc[u_taxa][j],str) or isinstance(df.loc[u_taxa][j],float) or isinstance(df.loc[u_taxa][j],int):\n",
    "                        taxa_abundance = float(df.loc[u_taxa][j])\n",
    "                    else:\n",
    "                        taxa_ablist = list(df.loc[u_taxa][j])\n",
    "                        taxa_ablist = [float(x) for x in taxa_ablist]\n",
    "                        taxa_abundance = np.sum(taxa_ablist)\n",
    "                    sample_table.append(taxa_abundance)\n",
    "                else:\n",
    "                    sample_table.append(float(0))\n",
    "\n",
    "            merge_table.append(sample_table)\n",
    "        print(f'file {file} screening completed\\n')\n",
    "    t_merge_table = np.transpose(merge_table).tolist()\n",
    "    t_merge_df = pd.DataFrame(t_merge_table,columns = sample_list)\n",
    "    # replace by your own path\n",
    "    t_merge_df.to_csv(\"D:/ShanxiJiankang/2021_shanghai_household_data/multiomics/merge_indoor_countfile.txt\",sep=\"\\t\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # merge count matrix, mostly unused\n",
    "    if  not os.path.exists(\"D:/ShanxiJiankang/2021_shanghai_household_data/mice_gut/true_countfile_p1.txt\"):\n",
    "        merge_count_matrix(\"D:\\\\ShanxiJiankang\\\\2021_shanghai_household_data\\\\mice_gut\\\\read_count_p1\")\n",
    "    \n",
    "    #replace by your own meta file directory\n",
    "    metapath = \"D:\\\\ShanxiJiankang\\\\2021_shanghai_household_data\\\\another_analysis\\\\cohort_seq_meta\\\\6m_meta\\\\\"\n",
    "    meta_list = list(glob.glob(os.path.join(metapath,\"*.txt\")))\n",
    "    #replace by your own count matrix file path\n",
    "    countfile = \"D:/ShanxiJiankang/2021_shanghai_household_data/another_analysis/Species_counts.txt\"\n",
    "    for metafile in meta_list:\n",
    "        metadata = metafile\n",
    "        new_res = run_EM(countfile,metadata,1000)\n",
    "        sink_name = new_res['sink_name']\n",
    "        sink_type = new_res['sink_type']\n",
    "        sample_list = new_res['sample']\n",
    "        sample_list.append('unknown')\n",
    "        taxa_list = new_res['taxa']\n",
    "        candidate_source_df = pd.DataFrame(new_res['params'],columns=sample_list,index=taxa_list)\n",
    "        candidate_alpha_df = pd.DataFrame(new_res['alpha'],index=sample_list)\n",
    "        \n",
    "        ## output 1.alpha: proportion in different sources 2. source matrix: proportion in different sources \n",
    "        ## per taxa\n",
    "        #replace by your own source matrix file path\n",
    "        candidate_source_df.to_csv(\"D:/ShanxiJiankang/2021_shanghai_household_data/multiomics/source_res_indoor/\"+sink_name+\"_source.txt\",sep=\"\\t\")\n",
    "        #replace by your own alpha file path\n",
    "        candidate_alpha_df.to_csv(\"D:/ShanxiJiankang/2021_shanghai_household_data/multiomics/source_res_indoor/\"+sink_name+\"_alpha.txt\",sep=\"\\t\")\n",
    "main()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
